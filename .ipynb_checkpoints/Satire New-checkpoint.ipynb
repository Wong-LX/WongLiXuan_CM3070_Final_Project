{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e988bf",
   "metadata": {},
   "source": [
    "# Build dataset for training 'satire/not satire' classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8317963",
   "metadata": {},
   "source": [
    "## Scrap news articles from 'theonion.com' (satire news articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a11e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.theonion.com'\n",
    "r = requests.get(base+'/breaking-news/news?')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "s=Service('C:\\\\Users\\\\Li Xuan\\\\Desktop\\\\FYP\\\\chromedriver')\n",
    "browser = webdriver.Chrome(service=s)\n",
    "url='https://www.theonion.com/breaking-news/news'\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05184e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.theonion.com/breaking-news/news?startIndex='\n",
    "startIdx = 20;\n",
    "i = 1;\n",
    "linksToArticles = [];\n",
    "\n",
    "while(i < 101):\n",
    "    currentIdx = str( i * startIdx);\n",
    "    r = requests.get(base+currentIdx)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for entry in soup.find_all('a', href=True):\n",
    "        linksToArticles.append(entry['href'])\n",
    "        linksToArticles = list(set(linksToArticles))\n",
    "        \n",
    "    for link in linksToArticles:\n",
    "        if link[0] == '/' or link[0]=='?':\n",
    "            linksToArticles.remove(link)\n",
    "    i+=1;\n",
    "        \n",
    "\n",
    "linksToArticles.sort(key=len, reverse=True)\n",
    "linksToArticles = linksToArticles[:20 * (i-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all news articles from the links\n",
    "\n",
    "articles = []\n",
    "headlines = []\n",
    "\n",
    "for link in linksToArticles:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    article = \"\";\n",
    "    obj = soup.find_all(\"p\", class_=\"sc-77igqf-0 bOfvBY\")\n",
    "    headline = soup.find('h1', class_ = \"sc-1efpnfq-0 iQQrUa\").text\n",
    "\n",
    "    i = 0;\n",
    "    for entry in obj:\n",
    "        article+=entry.text;\n",
    "    articles.append(article);\n",
    "    headlines.append(headline);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save news articles from theonion.com to CSV file\n",
    "\n",
    "articles_dict = {\"title\": headlines,\"text\":articles}\n",
    "df = pd.DataFrame(articles_dict)\n",
    "df.to_csv('theOnionArticles.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "917af1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>PERIL ISLAND—Big-game hunter Baron Hugo von Ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>STANFORD, CA—Theorizing that their work most l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>WASHINGTON—In what many are hailing as the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>HALLANDALE, FL–Randy Streeter, a 32-year-old s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>COLUMBIA, SC—Archaeologists excavating the tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1  Scientists Make Unclear Breakthrough After Giv...   \n",
       "2  Astronomers Discover Planet Identical To Earth...   \n",
       "3  Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4  Archaeologists Discover Remnants Of Legendary ...   \n",
       "\n",
       "                                                text  \n",
       "0  PERIL ISLAND—Big-game hunter Baron Hugo von Ur...  \n",
       "1  STANFORD, CA—Theorizing that their work most l...  \n",
       "2  WASHINGTON—In what many are hailing as the mos...  \n",
       "3  HALLANDALE, FL–Randy Streeter, a 32-year-old s...  \n",
       "4  COLUMBIA, SC—Archaeologists excavating the tra...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from 'theOnionArticles.csv'\n",
    "\n",
    "onion_df = pd.read_csv(\"theOnionArticles.csv\", index_col=False)\n",
    "onion_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2aaed7",
   "metadata": {},
   "source": [
    "## Create dataframe for true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabf1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb58715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15061</th>\n",
       "      <td>Trump offers to mediate in South China Sea dis...</td>\n",
       "      <td>HANOI (Reuters) - U.S. President Donald Trump ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>France condemns latest North Korean missile la...</td>\n",
       "      <td>PARIS (Reuters) - France on Friday condemned t...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 15, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>For Trump, it was the lost art of the deal</td>\n",
       "      <td>WASHINGTON (Reuters) - In the end, the Closer ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>March 25, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>Pakistani diplomat shot in Afghanistan</td>\n",
       "      <td>JALALABAD, Afghanistan (Reuters) - A Pakistani...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 6, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>European states push U.S. for detailed Middle ...</td>\n",
       "      <td>UNITED NATIONS (Reuters) - Britain, France, Ge...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 8, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "15061  Trump offers to mediate in South China Sea dis...   \n",
       "19981  France condemns latest North Korean missile la...   \n",
       "4690          For Trump, it was the lost art of the deal   \n",
       "15552             Pakistani diplomat shot in Afghanistan   \n",
       "12680  European states push U.S. for detailed Middle ...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "15061  HANOI (Reuters) - U.S. President Donald Trump ...     worldnews   \n",
       "19981  PARIS (Reuters) - France on Friday condemned t...     worldnews   \n",
       "4690   WASHINGTON (Reuters) - In the end, the Closer ...  politicsNews   \n",
       "15552  JALALABAD, Afghanistan (Reuters) - A Pakistani...     worldnews   \n",
       "12680  UNITED NATIONS (Reuters) - Britain, France, Ge...     worldnews   \n",
       "\n",
       "                      date  \n",
       "15061   November 12, 2017   \n",
       "19981  September 15, 2017   \n",
       "4690       March 25, 2017   \n",
       "15552    November 6, 2017   \n",
       "12680    December 8, 2017   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'Kaggle dataset/True.csv'\n",
    "true_df = pd.read_csv(file_name, index_col=False)\n",
    "\n",
    "# Get 2000 random rows from the true news dataframe\n",
    "\n",
    "not_onion_df = pd.DataFrame()\n",
    "true_df_size = true_df.shape[0]\n",
    "randomIdx = random.sample(range(true_df_size), 2000)\n",
    "\n",
    "for idx in randomIdx:\n",
    "    entry = true_df.iloc[[idx]]\n",
    "    not_onion_df = not_onion_df.append(entry)\n",
    "    \n",
    "not_onion_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5e3aa",
   "metadata": {},
   "source": [
    "## Data cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b4259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import contractions\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# # required to tokenize strings\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# # required to perform parts-of-speech tagging\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# from nltk.corpus import wordnet, stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cacdb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove any rows from the 2 dataframes that have NaN values\n",
    "\n",
    "onion_df = onion_df.dropna()\n",
    "not_onion_df = not_onion_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a9a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove location precursor from each article\n",
    "\n",
    "for index in onion_df.index:\n",
    "    if \"—\" in onion_df.loc[index,\"text\"]:\n",
    "        updated = onion_df.loc[index,'text'].split(\"—\")\n",
    "        onion_df.loc[index,'text'] = updated[1]\n",
    "        \n",
    "\n",
    "for index in not_onion_df.index:\n",
    "    if \") - \" in not_onion_df.loc[index,\"text\"]:\n",
    "        updated = not_onion_df.loc[index,'text'].split(\") - \")\n",
    "        not_onion_df.loc[index,'text'] = updated[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93680aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>Historical Archives: Satan, Dark Harbinger Of ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title text\n",
       "1158  Historical Archives: Satan, Dark Harbinger Of ...     "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onion_df.iloc[[1156]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f497f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each entry in the dataframe whether satire or not\n",
    "\n",
    "onion_df['satire/not satire'] = 1\n",
    "not_onion_df['satire/not satire'] = 0\n",
    "\n",
    "not_onion_df = not_onion_df.drop(['subject','date'],axis = 1)\n",
    "not_onion_df = not_onion_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3616dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>Big-game hunter Baron Hugo von Urwitz conceded...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>Theorizing that their work most likely represe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>In what many are hailing as the most significa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>HALLANDALE, FL–Randy Streeter, a 32-year-old s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>Archaeologists excavating the train tracks out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1  Scientists Make Unclear Breakthrough After Giv...   \n",
       "2  Astronomers Discover Planet Identical To Earth...   \n",
       "3  Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4  Archaeologists Discover Remnants Of Legendary ...   \n",
       "\n",
       "                                                text  satire/not satire  \n",
       "0  Big-game hunter Baron Hugo von Urwitz conceded...                  1  \n",
       "1  Theorizing that their work most likely represe...                  1  \n",
       "2  In what many are hailing as the most significa...                  1  \n",
       "3  HALLANDALE, FL–Randy Streeter, a 32-year-old s...                  1  \n",
       "4  Archaeologists excavating the train tracks out...                  1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e130a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15061</td>\n",
       "      <td>Trump offers to mediate in South China Sea dis...</td>\n",
       "      <td>U.S. President Donald Trump said on Sunday tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19981</td>\n",
       "      <td>France condemns latest North Korean missile la...</td>\n",
       "      <td>France on Friday condemned the latest missile ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4690</td>\n",
       "      <td>For Trump, it was the lost art of the deal</td>\n",
       "      <td>In the end, the Closer couldn’t close the deal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15552</td>\n",
       "      <td>Pakistani diplomat shot in Afghanistan</td>\n",
       "      <td>A Pakistani diplomatic official was shot by un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12680</td>\n",
       "      <td>European states push U.S. for detailed Middle ...</td>\n",
       "      <td>Britain, France, Germany, Sweden and Italy cal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                              title  \\\n",
       "0  15061  Trump offers to mediate in South China Sea dis...   \n",
       "1  19981  France condemns latest North Korean missile la...   \n",
       "2   4690         For Trump, it was the lost art of the deal   \n",
       "3  15552             Pakistani diplomat shot in Afghanistan   \n",
       "4  12680  European states push U.S. for detailed Middle ...   \n",
       "\n",
       "                                                text  satire/not satire  \n",
       "0  U.S. President Donald Trump said on Sunday tha...                  0  \n",
       "1  France on Friday condemned the latest missile ...                  0  \n",
       "2  In the end, the Closer couldn’t close the deal...                  0  \n",
       "3  A Pakistani diplomatic official was shot by un...                  0  \n",
       "4  Britain, France, Germany, Sweden and Italy cal...                  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_onion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e697c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58787531",
   "metadata": {},
   "source": [
    "## Combine dataframes of true news and 'The Onion' news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "775bca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>Big-game hunter Baron Hugo von Urwitz conceded...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>Theorizing that their work most likely represe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>In what many are hailing as the most significa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>HALLANDALE, FL–Randy Streeter, a 32-year-old s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>Archaeologists excavating the train tracks out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>U.S. senators say Cuba's Castro keen to contin...</td>\n",
       "      <td>Cuban President Raul Castro made it clear to a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Italy's 5-Star launches vote for leader, Di Ma...</td>\n",
       "      <td>Italy s anti-establishment 5-Star Movement, wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>In rural-urban divide, U.S. voters are worlds ...</td>\n",
       "      <td>Semi-retired Wisconsin pig farmer John Lader d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Airlines in Cairo asked to implement Trump tra...</td>\n",
       "      <td>Airlines operating at Cairo airport were offic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Michigan governor issues appeal over Flint fun...</td>\n",
       "      <td>Michigan Governor Rick Snyder has urged federa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1     Scientists Make Unclear Breakthrough After Giv...   \n",
       "2     Astronomers Discover Planet Identical To Earth...   \n",
       "3     Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4     Archaeologists Discover Remnants Of Legendary ...   \n",
       "...                                                 ...   \n",
       "1995  U.S. senators say Cuba's Castro keen to contin...   \n",
       "1996  Italy's 5-Star launches vote for leader, Di Ma...   \n",
       "1997  In rural-urban divide, U.S. voters are worlds ...   \n",
       "1998  Airlines in Cairo asked to implement Trump tra...   \n",
       "1999  Michigan governor issues appeal over Flint fun...   \n",
       "\n",
       "                                                   text  satire/not satire  \n",
       "0     Big-game hunter Baron Hugo von Urwitz conceded...                  1  \n",
       "1     Theorizing that their work most likely represe...                  1  \n",
       "2     In what many are hailing as the most significa...                  1  \n",
       "3     HALLANDALE, FL–Randy Streeter, a 32-year-old s...                  1  \n",
       "4     Archaeologists excavating the train tracks out...                  1  \n",
       "...                                                 ...                ...  \n",
       "1995  Cuban President Raul Castro made it clear to a...                  0  \n",
       "1996  Italy s anti-establishment 5-Star Movement, wh...                  0  \n",
       "1997  Semi-retired Wisconsin pig farmer John Lader d...                  0  \n",
       "1998  Airlines operating at Cairo airport were offic...                  0  \n",
       "1999  Michigan Governor Rick Snyder has urged federa...                  0  \n",
       "\n",
       "[3994 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df = pd.DataFrame()\n",
    "satire_df = pd.concat([onion_df, not_onion_df], axis=0)\n",
    "satire_df=satire_df.drop(['index'],axis=1)\n",
    "\n",
    "satire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41cd5298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>Big-game hunter Baron Hugo von Urwitz conceded...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>Theorizing that their work most likely represe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>In what many are hailing as the most significa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>HALLANDALE, FL–Randy Streeter, a 32-year-old s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>Archaeologists excavating the train tracks out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>U.S. senators say Cuba's Castro keen to contin...</td>\n",
       "      <td>Cuban President Raul Castro made it clear to a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Italy's 5-Star launches vote for leader, Di Ma...</td>\n",
       "      <td>Italy s anti-establishment 5-Star Movement, wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>In rural-urban divide, U.S. voters are worlds ...</td>\n",
       "      <td>Semi-retired Wisconsin pig farmer John Lader d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>Airlines in Cairo asked to implement Trump tra...</td>\n",
       "      <td>Airlines operating at Cairo airport were offic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>Michigan governor issues appeal over Flint fun...</td>\n",
       "      <td>Michigan Governor Rick Snyder has urged federa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1     Scientists Make Unclear Breakthrough After Giv...   \n",
       "2     Astronomers Discover Planet Identical To Earth...   \n",
       "3     Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4     Archaeologists Discover Remnants Of Legendary ...   \n",
       "...                                                 ...   \n",
       "3989  U.S. senators say Cuba's Castro keen to contin...   \n",
       "3990  Italy's 5-Star launches vote for leader, Di Ma...   \n",
       "3991  In rural-urban divide, U.S. voters are worlds ...   \n",
       "3992  Airlines in Cairo asked to implement Trump tra...   \n",
       "3993  Michigan governor issues appeal over Flint fun...   \n",
       "\n",
       "                                                   text  satire/not satire  \n",
       "0     Big-game hunter Baron Hugo von Urwitz conceded...                  1  \n",
       "1     Theorizing that their work most likely represe...                  1  \n",
       "2     In what many are hailing as the most significa...                  1  \n",
       "3     HALLANDALE, FL–Randy Streeter, a 32-year-old s...                  1  \n",
       "4     Archaeologists excavating the train tracks out...                  1  \n",
       "...                                                 ...                ...  \n",
       "3989  Cuban President Raul Castro made it clear to a...                  0  \n",
       "3990  Italy s anti-establishment 5-Star Movement, wh...                  0  \n",
       "3991  Semi-retired Wisconsin pig farmer John Lader d...                  0  \n",
       "3992  Airlines operating at Cairo airport were offic...                  0  \n",
       "3993  Michigan Governor Rick Snyder has urged federa...                  0  \n",
       "\n",
       "[3994 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index\n",
    "satire_df=satire_df.reset_index(drop=True)\n",
    "\n",
    "satire_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0126b",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd71bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "sns.countplot(x='satire/not satire', hue='satire/not satire', data=satire_df, palette=\"Set1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507955e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a wordcloud object to get a visualization of the distribution of words in the corpus\n",
    "\n",
    "words = ' '.join([text for text in satire_df['text']])\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color='black', max_font_size=200).generate(words)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.title('All words', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e90b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency of each words. For creating custom stopwords list\n",
    "freq_dict = WordCloud().process_text(words)\n",
    "\n",
    "# sort the dictionary from most frequently occuring to least frequently occuring\n",
    "freq_dict_sorted = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# shortlist the 5 most frequently occuring words\n",
    "print('The 5 most frequently occuring words in the dataframe are:')\n",
    "for idx in range(len(freq_dict_sorted[:5])):\n",
    "    print(freq_dict_sorted[idx][0], \"-\", freq_dict_sorted[idx][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fcdcb",
   "metadata": {},
   "source": [
    "## Tokenization and Stopwords removal\n",
    "\n",
    "On removing stopwords, dataset size decreases and the time to train the model also decreases\n",
    "Removing stopwords can potentially help improve the performance as there are fewer and only meaningful tokens left. Thus, it could increase classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194137a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52484202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "      <th>tokenized text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>big-game hunter baron hugo von urwitz conceded...</td>\n",
       "      <td>1</td>\n",
       "      <td>[big, game, hunter, baron, hugo, von, urwitz, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>theorizing that their work most likely represe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[theorizing, that, their, work, most, likely, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>in what many are hailing as the most significa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[in, what, many, are, hailing, as, the, most, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>hallandale, fl–randy streeter, a 32-year-old s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hallandale, fl, randy, streeter, a, 32, year,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>archaeologists excavating the train tracks out...</td>\n",
       "      <td>1</td>\n",
       "      <td>[archaeologists, excavating, the, train, track...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>U.S. senators say Cuba's Castro keen to contin...</td>\n",
       "      <td>cuban president raul castro made it clear to a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cuban, president, raul, castro, made, it, cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Italy's 5-Star launches vote for leader, Di Ma...</td>\n",
       "      <td>italy s anti-establishment 5-star movement, wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[italy, s, anti, establishment, 5, star, movem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>In rural-urban divide, U.S. voters are worlds ...</td>\n",
       "      <td>semi-retired wisconsin pig farmer john lader d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[semi, retired, wisconsin, pig, farmer, john, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>Airlines in Cairo asked to implement Trump tra...</td>\n",
       "      <td>airlines operating at cairo airport were offic...</td>\n",
       "      <td>0</td>\n",
       "      <td>[airlines, operating, at, cairo, airport, were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>Michigan governor issues appeal over Flint fun...</td>\n",
       "      <td>michigan governor rick snyder has urged federa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[michigan, governor, rick, snyder, has, urged,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3994 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1     Scientists Make Unclear Breakthrough After Giv...   \n",
       "2     Astronomers Discover Planet Identical To Earth...   \n",
       "3     Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4     Archaeologists Discover Remnants Of Legendary ...   \n",
       "...                                                 ...   \n",
       "3989  U.S. senators say Cuba's Castro keen to contin...   \n",
       "3990  Italy's 5-Star launches vote for leader, Di Ma...   \n",
       "3991  In rural-urban divide, U.S. voters are worlds ...   \n",
       "3992  Airlines in Cairo asked to implement Trump tra...   \n",
       "3993  Michigan governor issues appeal over Flint fun...   \n",
       "\n",
       "                                                   text  satire/not satire  \\\n",
       "0     big-game hunter baron hugo von urwitz conceded...                  1   \n",
       "1     theorizing that their work most likely represe...                  1   \n",
       "2     in what many are hailing as the most significa...                  1   \n",
       "3     hallandale, fl–randy streeter, a 32-year-old s...                  1   \n",
       "4     archaeologists excavating the train tracks out...                  1   \n",
       "...                                                 ...                ...   \n",
       "3989  cuban president raul castro made it clear to a...                  0   \n",
       "3990  italy s anti-establishment 5-star movement, wh...                  0   \n",
       "3991  semi-retired wisconsin pig farmer john lader d...                  0   \n",
       "3992  airlines operating at cairo airport were offic...                  0   \n",
       "3993  michigan governor rick snyder has urged federa...                  0   \n",
       "\n",
       "                                         tokenized text  \n",
       "0     [big, game, hunter, baron, hugo, von, urwitz, ...  \n",
       "1     [theorizing, that, their, work, most, likely, ...  \n",
       "2     [in, what, many, are, hailing, as, the, most, ...  \n",
       "3     [hallandale, fl, randy, streeter, a, 32, year,...  \n",
       "4     [archaeologists, excavating, the, train, track...  \n",
       "...                                                 ...  \n",
       "3989  [cuban, president, raul, castro, made, it, cle...  \n",
       "3990  [italy, s, anti, establishment, 5, star, movem...  \n",
       "3991  [semi, retired, wisconsin, pig, farmer, john, ...  \n",
       "3992  [airlines, operating, at, cairo, airport, were...  \n",
       "3993  [michigan, governor, rick, snyder, has, urged,...  \n",
       "\n",
       "[3994 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "satire_df['text'] = [text.lower() for text in satire_df['text']]\n",
    "\n",
    "#Convert all text to word tokens\n",
    "satire_df['tokenized text'] = [tokenizer.tokenize(text) for text in satire_df['text']]\n",
    "\n",
    "satire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2546b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>000270</th>\n",
       "      <th>0003</th>\n",
       "      <th>00042</th>\n",
       "      <th>0005</th>\n",
       "      <th>000938</th>\n",
       "      <th>000s</th>\n",
       "      <th>000th</th>\n",
       "      <th>...</th>\n",
       "      <th>zzz</th>\n",
       "      <th>æthelmearc</th>\n",
       "      <th>éclaires</th>\n",
       "      <th>émigré</th>\n",
       "      <th>émile</th>\n",
       "      <th>époque</th>\n",
       "      <th>état</th>\n",
       "      <th>östersund</th>\n",
       "      <th>übermenschen</th>\n",
       "      <th>ÿ92</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3994 rows × 47965 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0000  000270  0003  00042  0005  000938  000s  000th  ...  zzz  \\\n",
       "0      0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "1      0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "2      0    2     0       0     0      0     0       0     0      0  ...    0   \n",
       "3      0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "4      0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "...   ..  ...   ...     ...   ...    ...   ...     ...   ...    ...  ...  ...   \n",
       "3989   0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "3990   0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "3991   0    2     0       0     0      0     0       0     0      0  ...    0   \n",
       "3992   0    0     0       0     0      0     0       0     0      0  ...    0   \n",
       "3993   0    1     0       0     0      0     0       0     0      0  ...    0   \n",
       "\n",
       "      æthelmearc  éclaires  émigré  émile  époque  état  östersund  \\\n",
       "0              0         0       0      0       0     0          0   \n",
       "1              0         0       0      0       0     0          0   \n",
       "2              0         0       0      0       0     0          0   \n",
       "3              0         0       0      0       0     0          0   \n",
       "4              0         0       0      0       0     0          0   \n",
       "...          ...       ...     ...    ...     ...   ...        ...   \n",
       "3989           0         0       0      0       0     0          0   \n",
       "3990           0         0       0      0       0     0          0   \n",
       "3991           0         0       0      0       0     0          0   \n",
       "3992           0         0       0      0       0     0          0   \n",
       "3993           0         0       0      0       0     0          0   \n",
       "\n",
       "      übermenschen  ÿ92  \n",
       "0                0    0  \n",
       "1                0    0  \n",
       "2                0    0  \n",
       "3                0    0  \n",
       "4                0    0  \n",
       "...            ...  ...  \n",
       "3989             0    0  \n",
       "3990             0    0  \n",
       "3991             0    0  \n",
       "3992             0    0  \n",
       "3993             0    0  \n",
       "\n",
       "[3994 rows x 47965 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bag-of-words model\n",
    "vectorizer = CountVectorizer()\n",
    "all_docs = list(satire_df['text'])\n",
    "\n",
    "X = vectorizer.fit_transform(all_docs).toarray()\n",
    "bow = pd.DataFrame(X, columns = vectorizer.get_feature_names())\n",
    "\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1fc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "custom_stop_words = ['U S', 'said', 'will', 'one', 'people']\n",
    "stop_words.extend(custom_stop_words)\n",
    "\n",
    "# remove stopwords from all articles\n",
    "satire_df['stopwords removed'] = satire_df['tokenized text'].apply(lambda tokens: [token for token in tokens if token not in stop_words])\n",
    "\n",
    "# remove numeric values from all articles\n",
    "satire_df['stopwords removed'] = satire_df['stopwords removed'].apply(lambda tokens: [token for token in tokens if not token.isdigit()])\n",
    "\n",
    "# remove words that are not improper (contains numbers in them)\n",
    "satire_df['stopwords removed'] = satire_df['stopwords removed'].apply(lambda tokens: [token for token in tokens if not bool(re.search(r'\\d', token))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22ff2a",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f11c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to attempt to convert each word to its root morpheme\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "satire_df['lemmatized'] = satire_df['stopwords removed'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "556df572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join back tokens to get back original string from each list of tokens\n",
    "satire_df['processed text'] = satire_df['lemmatized'].apply(lambda lemmatizedList: ' '.join([word for word in lemmatizedList]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b97b410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaaarrrrgggghhh</th>\n",
       "      <th>aacr</th>\n",
       "      <th>aaf</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahed</th>\n",
       "      <th>aal</th>\n",
       "      <th>aao</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapp</th>\n",
       "      <th>...</th>\n",
       "      <th>zyuganov</th>\n",
       "      <th>zzz</th>\n",
       "      <th>æthelmearc</th>\n",
       "      <th>éclaires</th>\n",
       "      <th>émigré</th>\n",
       "      <th>émile</th>\n",
       "      <th>époque</th>\n",
       "      <th>état</th>\n",
       "      <th>östersund</th>\n",
       "      <th>übermenschen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3994 rows × 41076 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaaaaaarrrrgggghhh  aacr  aaf  aah  aahed  aal  aao  aapl  aapp  \\\n",
       "0      0                   0     0    0    0      0    0    0     0     0   \n",
       "1      0                   0     0    0    0      0    0    0     0     0   \n",
       "2      0                   0     0    0    0      0    0    0     0     0   \n",
       "3      0                   0     0    0    0      0    0    0     0     0   \n",
       "4      0                   0     0    0    0      0    0    0     0     0   \n",
       "...   ..                 ...   ...  ...  ...    ...  ...  ...   ...   ...   \n",
       "3989   0                   0     0    0    0      0    0    0     0     0   \n",
       "3990   0                   0     0    0    0      0    0    0     0     0   \n",
       "3991   0                   0     0    0    0      0    0    0     0     0   \n",
       "3992   0                   0     0    0    0      0    0    0     0     0   \n",
       "3993   0                   0     0    0    0      0    0    0     0     0   \n",
       "\n",
       "      ...  zyuganov  zzz  æthelmearc  éclaires  émigré  émile  époque  état  \\\n",
       "0     ...         0    0           0         0       0      0       0     0   \n",
       "1     ...         0    0           0         0       0      0       0     0   \n",
       "2     ...         0    0           0         0       0      0       0     0   \n",
       "3     ...         0    0           0         0       0      0       0     0   \n",
       "4     ...         0    0           0         0       0      0       0     0   \n",
       "...   ...       ...  ...         ...       ...     ...    ...     ...   ...   \n",
       "3989  ...         0    0           0         0       0      0       0     0   \n",
       "3990  ...         0    0           0         0       0      0       0     0   \n",
       "3991  ...         0    0           0         0       0      0       0     0   \n",
       "3992  ...         0    0           0         0       0      0       0     0   \n",
       "3993  ...         0    0           0         0       0      0       0     0   \n",
       "\n",
       "      östersund  übermenschen  \n",
       "0             0             0  \n",
       "1             0             0  \n",
       "2             0             0  \n",
       "3             0             0  \n",
       "4             0             0  \n",
       "...         ...           ...  \n",
       "3989          0             0  \n",
       "3990          0             0  \n",
       "3991          0             0  \n",
       "3992          0             0  \n",
       "3993          0             0  \n",
       "\n",
       "[3994 rows x 41076 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bag-of-words model\n",
    "all_docs = list(satire_df['processed text'])\n",
    "X = vectorizer.fit_transform(all_docs).toarray()\n",
    "bow = pd.DataFrame(X, columns = vectorizer.get_feature_names())\n",
    "\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3dcfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as csv file\n",
    "\n",
    "satire_df.to_csv('satire or not.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c7bfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "satire_df = pd.read_csv(\"satire or not.csv\", index_col=False)\n",
    "satire_df = satire_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e8299",
   "metadata": {},
   "source": [
    "# Build classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "006f81ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>stopwords removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Historical Archives: Satan, Dark Harbinger Of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>Graphic: Supreme Court roundup</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title text  \\\n",
       "1156  Historical Archives: Satan, Dark Harbinger Of ...  NaN   \n",
       "2193                     Graphic: Supreme Court roundup        \n",
       "\n",
       "      satire/not satire tokenized text stopwords removed lemmatized  \\\n",
       "1156                  1             []                []         []   \n",
       "2193                  0             []                []         []   \n",
       "\n",
       "     processed text  \n",
       "1156            NaN  \n",
       "2193            NaN  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire2_df[satire2_df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2804a7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>Historical Archives: Satan, Dark Harbinger Of ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title text  \\\n",
       "1158  Historical Archives: Satan, Dark Harbinger Of ...        \n",
       "\n",
       "      satire/not satire  \n",
       "1158                  1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onion_df.iloc[[1156]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6121fc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>stopwords removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Historical Archives: Satan, Dark Harbinger Of ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title text  \\\n",
       "1156  Historical Archives: Satan, Dark Harbinger Of ...        \n",
       "\n",
       "      satire/not satire tokenized text stopwords removed lemmatized  \\\n",
       "1156                  1             []                []         []   \n",
       "\n",
       "     processed text  \n",
       "1156                 "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df.iloc[[1156]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b7cf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ddee57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>stopwords removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>big-game hunter baron hugo von urwitz conceded...</td>\n",
       "      <td>1</td>\n",
       "      <td>[big, game, hunter, baron, hugo, von, urwitz, ...</td>\n",
       "      <td>[big, game, hunter, baron, hugo, von, urwitz, ...</td>\n",
       "      <td>[big, game, hunter, baron, hugo, von, urwitz, ...</td>\n",
       "      <td>big game hunter baron hugo von urwitz conceded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>theorizing that their work most likely represe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[theorizing, that, their, work, most, likely, ...</td>\n",
       "      <td>[theorizing, work, likely, represents, groundb...</td>\n",
       "      <td>[theorizing, work, likely, represents, groundb...</td>\n",
       "      <td>theorizing work likely represents groundbreaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>in what many are hailing as the most significa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[in, what, many, are, hailing, as, the, most, ...</td>\n",
       "      <td>[many, hailing, significant, development, hist...</td>\n",
       "      <td>[many, hailing, significant, development, hist...</td>\n",
       "      <td>many hailing significant development history s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>hallandale, fl–randy streeter, a 32-year-old s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hallandale, fl, randy, streeter, a, 32, year,...</td>\n",
       "      <td>[hallandale, fl, randy, streeter, year, old, s...</td>\n",
       "      <td>[hallandale, fl, randy, streeter, year, old, s...</td>\n",
       "      <td>hallandale fl randy streeter year old sale ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>archaeologists excavating the train tracks out...</td>\n",
       "      <td>1</td>\n",
       "      <td>[archaeologists, excavating, the, train, track...</td>\n",
       "      <td>[archaeologists, excavating, train, tracks, qu...</td>\n",
       "      <td>[archaeologist, excavating, train, track, quar...</td>\n",
       "      <td>archaeologist excavating train track quarry an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>Trump chooses congressman, former SEAL Zinke a...</td>\n",
       "      <td>president-elect donald trump has chosen first-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[president, elect, donald, trump, has, chosen,...</td>\n",
       "      <td>[president, elect, donald, trump, chosen, firs...</td>\n",
       "      <td>[president, elect, donald, trump, chosen, firs...</td>\n",
       "      <td>president elect donald trump chosen first term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>China complains to Australia over Turnbull com...</td>\n",
       "      <td>china said on friday it had lodged a complaint...</td>\n",
       "      <td>0</td>\n",
       "      <td>[china, said, on, friday, it, had, lodged, a, ...</td>\n",
       "      <td>[china, friday, lodged, complaint, australia, ...</td>\n",
       "      <td>[china, friday, lodged, complaint, australia, ...</td>\n",
       "      <td>china friday lodged complaint australia prime ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>Chinese fans prepare to welcome rich, powerful...</td>\n",
       "      <td>he may be a divisive figure back home, but u.s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[he, may, be, a, divisive, figure, back, home,...</td>\n",
       "      <td>[may, divisive, figure, back, home, u, preside...</td>\n",
       "      <td>[may, divisive, figure, back, home, u, preside...</td>\n",
       "      <td>may divisive figure back home u president dona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>Russia to help Syria rebuild energy facilities...</td>\n",
       "      <td>russia will be the only country to take part i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[russia, will, be, the, only, country, to, tak...</td>\n",
       "      <td>[russia, country, take, part, rebuilding, syri...</td>\n",
       "      <td>[russia, country, take, part, rebuilding, syri...</td>\n",
       "      <td>russia country take part rebuilding syrian ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>No proof Russian hacking influenced U.S. elect...</td>\n",
       "      <td>no evidence has emerged to suggest russian hac...</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, evidence, has, emerged, to, suggest, russ...</td>\n",
       "      <td>[evidence, emerged, suggest, russian, hacking,...</td>\n",
       "      <td>[evidence, emerged, suggest, russian, hacking,...</td>\n",
       "      <td>evidence emerged suggest russian hacking influ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3994 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1     Scientists Make Unclear Breakthrough After Giv...   \n",
       "2     Astronomers Discover Planet Identical To Earth...   \n",
       "3     Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4     Archaeologists Discover Remnants Of Legendary ...   \n",
       "...                                                 ...   \n",
       "3989  Trump chooses congressman, former SEAL Zinke a...   \n",
       "3990  China complains to Australia over Turnbull com...   \n",
       "3991  Chinese fans prepare to welcome rich, powerful...   \n",
       "3992  Russia to help Syria rebuild energy facilities...   \n",
       "3993  No proof Russian hacking influenced U.S. elect...   \n",
       "\n",
       "                                                   text  satire/not satire  \\\n",
       "0     big-game hunter baron hugo von urwitz conceded...                  1   \n",
       "1     theorizing that their work most likely represe...                  1   \n",
       "2     in what many are hailing as the most significa...                  1   \n",
       "3     hallandale, fl–randy streeter, a 32-year-old s...                  1   \n",
       "4     archaeologists excavating the train tracks out...                  1   \n",
       "...                                                 ...                ...   \n",
       "3989  president-elect donald trump has chosen first-...                  0   \n",
       "3990  china said on friday it had lodged a complaint...                  0   \n",
       "3991  he may be a divisive figure back home, but u.s...                  0   \n",
       "3992  russia will be the only country to take part i...                  0   \n",
       "3993  no evidence has emerged to suggest russian hac...                  0   \n",
       "\n",
       "                                         tokenized text  \\\n",
       "0     [big, game, hunter, baron, hugo, von, urwitz, ...   \n",
       "1     [theorizing, that, their, work, most, likely, ...   \n",
       "2     [in, what, many, are, hailing, as, the, most, ...   \n",
       "3     [hallandale, fl, randy, streeter, a, 32, year,...   \n",
       "4     [archaeologists, excavating, the, train, track...   \n",
       "...                                                 ...   \n",
       "3989  [president, elect, donald, trump, has, chosen,...   \n",
       "3990  [china, said, on, friday, it, had, lodged, a, ...   \n",
       "3991  [he, may, be, a, divisive, figure, back, home,...   \n",
       "3992  [russia, will, be, the, only, country, to, tak...   \n",
       "3993  [no, evidence, has, emerged, to, suggest, russ...   \n",
       "\n",
       "                                      stopwords removed  \\\n",
       "0     [big, game, hunter, baron, hugo, von, urwitz, ...   \n",
       "1     [theorizing, work, likely, represents, groundb...   \n",
       "2     [many, hailing, significant, development, hist...   \n",
       "3     [hallandale, fl, randy, streeter, year, old, s...   \n",
       "4     [archaeologists, excavating, train, tracks, qu...   \n",
       "...                                                 ...   \n",
       "3989  [president, elect, donald, trump, chosen, firs...   \n",
       "3990  [china, friday, lodged, complaint, australia, ...   \n",
       "3991  [may, divisive, figure, back, home, u, preside...   \n",
       "3992  [russia, country, take, part, rebuilding, syri...   \n",
       "3993  [evidence, emerged, suggest, russian, hacking,...   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0     [big, game, hunter, baron, hugo, von, urwitz, ...   \n",
       "1     [theorizing, work, likely, represents, groundb...   \n",
       "2     [many, hailing, significant, development, hist...   \n",
       "3     [hallandale, fl, randy, streeter, year, old, s...   \n",
       "4     [archaeologist, excavating, train, track, quar...   \n",
       "...                                                 ...   \n",
       "3989  [president, elect, donald, trump, chosen, firs...   \n",
       "3990  [china, friday, lodged, complaint, australia, ...   \n",
       "3991  [may, divisive, figure, back, home, u, preside...   \n",
       "3992  [russia, country, take, part, rebuilding, syri...   \n",
       "3993  [evidence, emerged, suggest, russian, hacking,...   \n",
       "\n",
       "                                         processed text  \n",
       "0     big game hunter baron hugo von urwitz conceded...  \n",
       "1     theorizing work likely represents groundbreaki...  \n",
       "2     many hailing significant development history s...  \n",
       "3     hallandale fl randy streeter year old sale ass...  \n",
       "4     archaeologist excavating train track quarry an...  \n",
       "...                                                 ...  \n",
       "3989  president elect donald trump chosen first term...  \n",
       "3990  china friday lodged complaint australia prime ...  \n",
       "3991  may divisive figure back home u president dona...  \n",
       "3992  russia country take part rebuilding syrian ene...  \n",
       "3993  evidence emerged suggest russian hacking influ...  \n",
       "\n",
       "[3994 rows x 7 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df = satire_df.dropna()\n",
    "\n",
    "satire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49b56328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus from all preprocessed tweets in corpus\n",
    "# all_docs = list(satire_df['processed text'])\n",
    "\n",
    "satire_df = pd.read_csv(\"satire or not.csv\", index_col=False)\n",
    "satire_df = satire_df.dropna()\n",
    "\n",
    "# split dataset into training and testing dataset\n",
    "X = list(satire_df['processed text'])\n",
    "Y = list(satire_df['satire/not satire'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "83f3fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the baseline model is: 0.9674\n"
     ]
    }
   ],
   "source": [
    "# split dataset into training and testing dataset\n",
    "X = list(satire_df['processed text'])\n",
    "Y = list(satire_df['satire/not satire'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state = 0)\n",
    "\n",
    "# Implement the multinomial NB classifier \n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "# Initialize count vectorizer with default hyperparameter values\n",
    "count_v = CountVectorizer()\n",
    "\n",
    "# convert training data into a bag of words model\n",
    "x_train_count = count_v.fit_transform(x_train)\n",
    "\n",
    "# train model\n",
    "mnb_classifier.fit(x_train_count, y_train)\n",
    "\n",
    "# make prediction\n",
    "x_test_count = count_v.transform(x_test)\n",
    "y_pred_count = mnb_classifier.predict(x_test_count)\n",
    "\n",
    "print(\"The accuracy of the baseline model is: \" + str(round(accuracy_score(y_test, y_pred_count), 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db8d63",
   "metadata": {},
   "source": [
    "Vectorization is the process of feature building, by turning text into numerical vectors. In text\n",
    "processing, words of the articles represent categorical features. This is a crucial step, as machine\n",
    "learning algorithms are unable to understand plain text. The most commonly used methods for this\n",
    "task are the Count Vectorizer and the Tf-IDF Vectorizer. Using a Count Vectorizer creates a Bag of\n",
    "words that counts the frequency of each word. However, using a Tf-IDF Vectorizer increases the value\n",
    "of the word proportionally to its count in a document, but is inversely proportional to its frequency in\n",
    "the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236c716",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6011640",
   "metadata": {},
   "source": [
    "### baseline model\n",
    "\n",
    "With countVectorizer, we first build a baseline Naive Bayes classifier using the default hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1391f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the baseline model is: 0.97269\n"
     ]
    }
   ],
   "source": [
    "# Implement the multinomial NB classifier \n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "# Initialize count vectorizer with default hyperparameter values\n",
    "count_v = CountVectorizer()\n",
    "\n",
    "# convert training data into a bag of words model\n",
    "x_train_count = count_v.fit_transform(x_train)\n",
    "\n",
    "# train model\n",
    "mnb_classifier.fit(x_train_count, y_train)\n",
    "\n",
    "# make prediction\n",
    "x_test_count = count_v.transform(x_test)\n",
    "y_pred_count = mnb_classifier.predict(x_test_count)\n",
    "\n",
    "print(\"The accuracy of the baseline model is: \" + str(round(accuracy_score(y_test, y_pred_count), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842034a",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5502bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df and max_df values hyperparameter values to be tuned\n",
    "maxDfs = [1.0, 0.99, 0.98, 0.97, 0.96]\n",
    "minDfs = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57891661",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ec7e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list to store results of each iteration of classifier with different hyperparameter settings\n",
    "mnb_cv_results = []\n",
    "\n",
    "best_cv = CountVectorizer()\n",
    "best_cv_acc = -1;\n",
    "\n",
    "# train classifier with different n-gram ranges, minDF and max_df\n",
    "for ngram_val in range(1,6):\n",
    "    for minDf_idx in range(len(minDfs)):\n",
    "        for maxDf_idx in range(len(maxDfs)):\n",
    "            count_v = CountVectorizer(ngram_range=(1,ngram_val), min_df = minDfs[minDf_idx], max_df = maxDfs[maxDf_idx])\n",
    "\n",
    "            # convert training data into a bag of words model\n",
    "            x_train_count = count_v.fit_transform(x_train)\n",
    "            x_test_count = count_v.transform(x_test)\n",
    "\n",
    "            # train model and generate predictions\n",
    "            mnb_classifier.fit(x_train_count, y_train)\n",
    "            y_pred_count = mnb_classifier.predict(x_test_count)\n",
    "            \n",
    "            # compute precision, recall, accuracy\n",
    "            recall = round(recall_score(y_test, y_pred_count),5)\n",
    "            precision = round(precision_score(y_test, y_pred_count),5)\n",
    "            acc = round(accuracy_score(y_test, y_pred_count),5)\n",
    "            \n",
    "            # If accuracy score is higher than the last highest accuracy score saved\n",
    "            if acc > best_cv_acc:\n",
    "                # replace last saved vectorizer with current one\n",
    "                best_cv = count_v\n",
    "                best_cv_acc = acc\n",
    "            \n",
    "            result = [minDfs[minDf_idx], maxDfs[maxDf_idx], ngram_val, recall, precision, acc]\n",
    "            mnb_cv_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b82fd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=2, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09282f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_df</th>\n",
       "      <th>ngam_range</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97872</td>\n",
       "      <td>0.98321</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97872</td>\n",
       "      <td>0.98321</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97872</td>\n",
       "      <td>0.98321</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97872</td>\n",
       "      <td>0.98321</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97872</td>\n",
       "      <td>0.98321</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_df  max_df  ngam_range   recall  precision  accuracy score\n",
       "31       2    0.99           2  0.97872    0.98321         0.98103\n",
       "32       2    0.98           2  0.97872    0.98321         0.98103\n",
       "33       2    0.97           2  0.97872    0.98321         0.98103\n",
       "34       2    0.96           2  0.97872    0.98321         0.98103\n",
       "30       2    1.00           2  0.97872    0.98321         0.98103"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_cv_df = pd.DataFrame (mnb_cv_results, columns = ['min_df', 'max_df', 'ngam_range', 'recall', 'precision', 'accuracy score'])\n",
    "mnb_cv_df = mnb_cv_df.sort_values(by=['accuracy score'], ascending=False)\n",
    "mnb_cv_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3c6e0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98558"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_classifier = MultinomialNB(alpha = 0)\n",
    "\n",
    "\n",
    "count_v = CountVectorizer(ngram_range=(1,2), min_df =2)\n",
    "\n",
    "# convert training data into a bag of words model\n",
    "x_train_count = count_v.fit_transform(x_train)\n",
    "x_test_count = count_v.transform(x_test)\n",
    "\n",
    "# train model and generate predictions\n",
    "mnb_classifier.fit(x_train_count, y_train)\n",
    "y_pred_count = mnb_classifier.predict(x_test_count)\n",
    "\n",
    "# compute precision, recall, accuracy\n",
    "recall = round(recall_score(y_test, y_pred_count),5)\n",
    "precision = round(precision_score(y_test, y_pred_count),5)\n",
    "acc = round(accuracy_score(y_test, y_pred_count),5)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1ef18",
   "metadata": {},
   "source": [
    "Out of 125 combinations of different min_df, max_df and gram_range values, the following combination used for the hyperparameters of CountVectorizer give the best accuracy score for the naive bayes classifier:\n",
    "    \n",
    "min_df = 2 <br>\n",
    "max_df = 0.96 (default) <br>\n",
    "ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fd605",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer\n",
    "\n",
    "With Tfidfvectorize on the contrary, you will do all three steps at once. Under the hood, it computes the word counts, IDF values, and TF-IDF scores all using the same data set.\n",
    "\n",
    "TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions.\n",
    "\n",
    "\n",
    "TFIDF is based on the logic that words that are too abundant in a corpus and words that are too rare are both not statistically important for finding a pattern. The Logarithmic factor in tfidf mathematically penalizes the words that are too abundant or too rare in the corpus by giving them low tfidf scores.\n",
    "\n",
    "Higher value of tfidf signifies higher importance of the words in the corpus while lower values represent lower importance. In the above example the word \"AI\" is present in both the sentences while words \"Natural\" and \"Computer\" are present only in one sentences each. Hence the tfidf value of \"AI\" is lower than the other two. While for the word \"Natural\" there are more words in Text1 hence its importance is lower than \"Computer\" since there are less number of words in Text2.\n",
    "\n",
    "\n",
    "Even though TFIDF can provide a good understanding about the importance of words but just like Count Vectors, its disadvantage is:\n",
    "\n",
    "It fails to provide linguistic information about the words such as the real meaning of the words, similarity with other words etc.\n",
    "\n",
    "To train a model on the actual linguistic relationship of the words, there are two other word embedding techniques widely used in NLP, they are \"word2vec\" and \"Glove\". I will discuss about these two in another article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "927d30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list to store results of each iteration of classifier with different hyperparameter settings\n",
    "mnb_tfidf_results = []\n",
    "\n",
    "best_tfidf_v = TfidfVectorizer()\n",
    "best_tfidf_v_acc = -1;\n",
    "\n",
    "# train classifier with different n-gram ranges, minDF and max_df\n",
    "for ngram_val in range(1,6):\n",
    "    for minDf_idx in range(len(minDfs)):\n",
    "        for maxDf_idx in range(len(maxDfs)):\n",
    "            tfidf_v = TfidfVectorizer(ngram_range=(1,ngram_val), min_df = minDfs[minDf_idx], max_df = maxDfs[maxDf_idx])\n",
    "\n",
    "            # convert training data into a bag of words model\n",
    "            x_train_tfidf = tfidf_v.fit_transform(x_train)\n",
    "            x_test_tfidf = tfidf_v.transform(x_test)\n",
    "\n",
    "            # train model and generate predictions\n",
    "            mnb_classifier.fit(x_train_tfidf, y_train)\n",
    "            y_pred_tfidf = mnb_classifier.predict(x_test_tfidf)\n",
    "            \n",
    "            # compute precision, recall, accuracy and f-1 score\n",
    "            recall = round(recall_score(y_test, y_pred_tfidf),5)\n",
    "            precision = round(precision_score(y_test, y_pred_tfidf),5)\n",
    "            acc = round(accuracy_score(y_test, y_pred_tfidf),5)\n",
    "            \n",
    "            # If accuracy score is higher than the highest accuracy score last saved\n",
    "            if acc > best_tfidf_v_acc:\n",
    "                # replace last saved vectorizer with current one\n",
    "                best_tfidf_v = tfidf_v\n",
    "                best_tfidf_v_acc = acc\n",
    "            \n",
    "            result = [minDfs[minDf_idx], maxDfs[maxDf_idx], ngram_val, recall, precision, acc]\n",
    "            mnb_tfidf_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d58e51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_df</th>\n",
       "      <th>ngam_range</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98313</td>\n",
       "      <td>0.97876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98313</td>\n",
       "      <td>0.97876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98313</td>\n",
       "      <td>0.97876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98313</td>\n",
       "      <td>0.97876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98313</td>\n",
       "      <td>0.97876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98162</td>\n",
       "      <td>0.97800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98162</td>\n",
       "      <td>0.97800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98162</td>\n",
       "      <td>0.97800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98162</td>\n",
       "      <td>0.97800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97416</td>\n",
       "      <td>0.98162</td>\n",
       "      <td>0.97800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_df  max_df  ngam_range   recall  precision  accuracy score\n",
       "31       2    0.99           2  0.97416    0.98313         0.97876\n",
       "32       2    0.98           2  0.97416    0.98313         0.97876\n",
       "33       2    0.97           2  0.97416    0.98313         0.97876\n",
       "34       2    0.96           2  0.97416    0.98313         0.97876\n",
       "30       2    1.00           2  0.97416    0.98313         0.97876\n",
       "40       4    1.00           2  0.97416    0.98162         0.97800\n",
       "36       3    0.99           2  0.97416    0.98162         0.97800\n",
       "37       3    0.98           2  0.97416    0.98162         0.97800\n",
       "38       3    0.97           2  0.97416    0.98162         0.97800\n",
       "39       3    0.96           2  0.97416    0.98162         0.97800"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_tfidf_df = pd.DataFrame (mnb_tfidf_results, columns = ['min_df', 'max_df', 'ngam_range', 'recall', 'precision', 'accuracy score'])\n",
    "mnb_tfidf_df = mnb_tfidf_df.sort_values(by=['accuracy score'], ascending=False)\n",
    "mnb_tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7eaf8e",
   "metadata": {},
   "source": [
    "Out of 125 combinations of different min_df, max_df and gram_range values, the following combination used for the hyperparameters of TfidfVectorizer give the best accuracy score for the naive bayes classifier:\n",
    "    \n",
    "min_df = 2 <br>\n",
    "max_df = 0.96 (default) <br>\n",
    "ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7c730",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb9e0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "tuned_mnb_results_count = []\n",
    "\n",
    "count_v = best_cv\n",
    "x_train_count = count_v.fit_transform(x_train)\n",
    "x_test_count = count_v.transform(x_test)\n",
    "\n",
    "best_cv_mnb = MultinomialNB();\n",
    "best_cv_mnb_acc = -1;\n",
    "\n",
    "for alpha_val in np.arange(0.0, 1.0, 0.1):\n",
    "    #Initialize Multinomial NB tuning classifier to accommodate incrementing alpha value\n",
    "    mnb_classifier_tuned = MultinomialNB(alpha = alpha_val, fit_prior = True)\n",
    "    #Fitting the X_train and y_train data into the classifier for training\n",
    "    mnb_classifier_tuned.fit(x_train_count,y_train)\n",
    "    #Predict the y_pred set using the X_test set using the tuning classifier\n",
    "    y_pred = mnb_classifier_tuned.predict(x_test_count)\n",
    "    \n",
    "    # compute precision, recall, accuracy and f-1 score\n",
    "    recall = round(recall_score(y_test, y_pred),5)\n",
    "    precision = round(precision_score(y_test, y_pred),5)\n",
    "    acc = round(accuracy_score(y_test, y_pred),5)\n",
    "    \n",
    "    # If accuracy score is higher than the highest accuracy score last saved\n",
    "    if acc > best_cv_mnb_acc:\n",
    "        # replace last saved vectorizer with current one\n",
    "        best_cv_mnb = mnb_classifier_tuned\n",
    "        best_cv_mnb_acc = acc\n",
    "\n",
    "    result = [alpha_val, recall, precision, acc]\n",
    "    tuned_mnb_results_count.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91f7e284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98328</td>\n",
       "      <td>0.98779</td>\n",
       "      <td>0.98558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.98480</td>\n",
       "      <td>0.98182</td>\n",
       "      <td>0.98331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.98328</td>\n",
       "      <td>0.98328</td>\n",
       "      <td>0.98331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98326</td>\n",
       "      <td>0.98255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98326</td>\n",
       "      <td>0.98255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98326</td>\n",
       "      <td>0.98255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98326</td>\n",
       "      <td>0.98255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.98024</td>\n",
       "      <td>0.98323</td>\n",
       "      <td>0.98179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.98024</td>\n",
       "      <td>0.98323</td>\n",
       "      <td>0.98179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha   recall  precision  accuracy score\n",
       "0    0.0  0.98328    0.98779         0.98558\n",
       "1    0.1  0.98480    0.98182         0.98331\n",
       "6    0.6  0.98328    0.98328         0.98331\n",
       "2    0.2  0.98176    0.98326         0.98255\n",
       "4    0.4  0.98176    0.98326         0.98255\n",
       "5    0.5  0.98176    0.98326         0.98255\n",
       "7    0.7  0.98176    0.98326         0.98255\n",
       "3    0.3  0.98176    0.98176         0.98179\n",
       "8    0.8  0.98024    0.98323         0.98179\n",
       "9    0.9  0.98024    0.98323         0.98179"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_mnb_df = pd.DataFrame (tuned_mnb_results_count, columns = ['alpha', 'recall', 'precision', 'accuracy score'])\n",
    "cv_mnb_df = cv_mnb_df.sort_values(by=['accuracy score'], ascending=False)\n",
    "cv_mnb_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06cf95de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "tuned_mnb_results_tfidf = []\n",
    "\n",
    "tfidf_v = TfidfVectorizer(ngram_range=(1,5), min_df = 2, max_df = 0.99)\n",
    "x_train_tfidf = tfidf_v.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_v.transform(x_test)\n",
    "\n",
    "\n",
    "best_tfidf_mnb = MultinomialNB();\n",
    "best_tfidf_mnb_acc = -1;\n",
    "\n",
    "for alpha_val in np.arange(0.0, 1.0, 0.1):\n",
    "    #Initialize Multinomial NB tuning classifier to accommodate incrementing alpha value\n",
    "    mnb_classifier_tuned = MultinomialNB(alpha = alpha_val, fit_prior = True)\n",
    "    #Fitting the X_train and y_train data into the classifier for training\n",
    "    mnb_classifier_tuned.fit(x_train_tfidf,y_train)\n",
    "    #Predict the y_pred set using the X_test set using the tuning classifier\n",
    "    y_pred = mnb_classifier_tuned.predict(x_test_tfidf)\n",
    "    \n",
    "    # compute precision, recall, accuracy and f-1 score\n",
    "    recall = round(recall_score(y_test, y_pred),5)\n",
    "    precision = round(precision_score(y_test, y_pred),5)\n",
    "    acc = round(accuracy_score(y_test, y_pred),5)\n",
    "    \n",
    "    # If accuracy score is higher than the highest accuracy score last saved\n",
    "    if acc > best_cv_mnb_acc:\n",
    "        # replace last saved vectorizer with current one\n",
    "        best_tfidf_mnb = mnb_classifier_tuned\n",
    "        best_tfidf_mnb_acc = acc\n",
    "    \n",
    "    result = [alpha_val, recall, precision, acc]\n",
    "    tuned_mnb_results_tfidf.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb9a0173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98176</td>\n",
       "      <td>0.98626</td>\n",
       "      <td>0.98407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.98936</td>\n",
       "      <td>0.97601</td>\n",
       "      <td>0.98255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.98784</td>\n",
       "      <td>0.97451</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.98784</td>\n",
       "      <td>0.97451</td>\n",
       "      <td>0.98103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.98632</td>\n",
       "      <td>0.97447</td>\n",
       "      <td>0.98027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.98480</td>\n",
       "      <td>0.97297</td>\n",
       "      <td>0.97876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.98328</td>\n",
       "      <td>0.97293</td>\n",
       "      <td>0.97800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.98328</td>\n",
       "      <td>0.97147</td>\n",
       "      <td>0.97724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.98024</td>\n",
       "      <td>0.97139</td>\n",
       "      <td>0.97572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.97872</td>\n",
       "      <td>0.97134</td>\n",
       "      <td>0.97496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha   recall  precision  accuracy score\n",
       "0    0.0  0.98176    0.98626         0.98407\n",
       "1    0.1  0.98936    0.97601         0.98255\n",
       "2    0.2  0.98784    0.97451         0.98103\n",
       "3    0.3  0.98784    0.97451         0.98103\n",
       "4    0.4  0.98632    0.97447         0.98027\n",
       "5    0.5  0.98480    0.97297         0.97876\n",
       "6    0.6  0.98328    0.97293         0.97800\n",
       "7    0.7  0.98328    0.97147         0.97724\n",
       "8    0.8  0.98024    0.97139         0.97572\n",
       "9    0.9  0.97872    0.97134         0.97496"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mnb_df = pd.DataFrame (tuned_mnb_results_tfidf, columns = ['alpha', 'recall', 'precision', 'accuracy score'])\n",
    "tfidf_mnb_df = tfidf_mnb_df.sort_values(by=['accuracy score'], ascending=False)\n",
    "tfidf_mnb_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811af1f",
   "metadata": {},
   "source": [
    "# k-folds cross validation\n",
    "Now let's use K-fold cross validation to objectively measure the accuracy of the classifier.\n",
    "https://bobbecket.github.io/06-naive-bayes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e919087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "messages = data['processed_text'].values\n",
    "counts = vectorizer.fit_transform(x_train)\n",
    "classes = satire_df['satire/not satire'].values\n",
    "\n",
    "\n",
    "scores = cross_val_score(classifier, counts, classes, cv=5)\n",
    "\n",
    "# Print the accuracy of each fold:\n",
    "print(scores)\n",
    "\n",
    "# Print the mean accuracy of all 5 folds\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"America Reminded Of Beef's Existence By Bold New Ad Campaign\"\n",
    "\n",
    "y_pred = mnb_classifier_tuned.predict(tfidf_v.transform([text]))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d63e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3a68f2",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "208336e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.1-py3-none-win_amd64.whl (89.1 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Installing collected packages: xgboost\n",
      "\n",
      "Successfully installed xgboost-1.7.1\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "66b99dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the relevant modules\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57488680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Parameters of the Model\n",
    "param = {'eta': 0.75,\n",
    "         'max_depth': 50,\n",
    "         'objective': 'binary:logitraw'}\n",
    "# Training the Model\n",
    "xgb_model = xgb.train(param, xgb_train, num_boost_round = 30)\n",
    "# Predicting using the Model\n",
    "y_pred = xgb_model.predict(xgb_test)\n",
    "y_pred = np.where(np.array(y_pred) > 0.5, 1, 0) # converting them to 1/0’s\n",
    "# Evaluation of Model\n",
    "accuracy_score(xgb_test_labels, y_pred)     # 92.47%\n",
    "f1_score(xgb_test_labels, y_pred)           # 94.83%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92a2b3",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b567866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84df25d1",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4634a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_cv_mnb.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_cv, 'best_mnb_vectorizer.pkl')\n",
    "joblib.dump(best_cv_mnb, 'best_cv_mnb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fae65976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>satire/not satire</th>\n",
       "      <th>tokenized text</th>\n",
       "      <th>stopwords removed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maverick Hunter's 'Human Beings As Prey' Plan ...</td>\n",
       "      <td>big-game hunter baron hugo von urwitz conceded...</td>\n",
       "      <td>1</td>\n",
       "      <td>['big', 'game', 'hunter', 'baron', 'hugo', 'vo...</td>\n",
       "      <td>['big', 'game', 'hunter', 'baron', 'hugo', 'vo...</td>\n",
       "      <td>['big', 'game', 'hunter', 'baron', 'hugo', 'vo...</td>\n",
       "      <td>big game hunter baron hugo von urwitz conceded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientists Make Unclear Breakthrough After Giv...</td>\n",
       "      <td>theorizing that their work most likely represe...</td>\n",
       "      <td>1</td>\n",
       "      <td>['theorizing', 'that', 'their', 'work', 'most'...</td>\n",
       "      <td>['theorizing', 'work', 'likely', 'represents',...</td>\n",
       "      <td>['theorizing', 'work', 'likely', 'represents',...</td>\n",
       "      <td>theorizing work likely represents groundbreaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astronomers Discover Planet Identical To Earth...</td>\n",
       "      <td>in what many are hailing as the most significa...</td>\n",
       "      <td>1</td>\n",
       "      <td>['in', 'what', 'many', 'are', 'hailing', 'as',...</td>\n",
       "      <td>['many', 'hailing', 'significant', 'developmen...</td>\n",
       "      <td>['many', 'hailing', 'significant', 'developmen...</td>\n",
       "      <td>many hailing significant development history s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Every Brand Of Alcohol Reminds Man Of A Differ...</td>\n",
       "      <td>hallandale, fl–randy streeter, a 32-year-old s...</td>\n",
       "      <td>1</td>\n",
       "      <td>['hallandale', 'fl', 'randy', 'streeter', 'a',...</td>\n",
       "      <td>['hallandale', 'fl', 'randy', 'streeter', 'yea...</td>\n",
       "      <td>['hallandale', 'fl', 'randy', 'streeter', 'yea...</td>\n",
       "      <td>hallandale fl randy streeter year old sale ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Archaeologists Discover Remnants Of Legendary ...</td>\n",
       "      <td>archaeologists excavating the train tracks out...</td>\n",
       "      <td>1</td>\n",
       "      <td>['archaeologists', 'excavating', 'the', 'train...</td>\n",
       "      <td>['archaeologists', 'excavating', 'train', 'tra...</td>\n",
       "      <td>['archaeologist', 'excavating', 'train', 'trac...</td>\n",
       "      <td>archaeologist excavating train track quarry an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Maverick Hunter's 'Human Beings As Prey' Plan ...   \n",
       "1  Scientists Make Unclear Breakthrough After Giv...   \n",
       "2  Astronomers Discover Planet Identical To Earth...   \n",
       "3  Every Brand Of Alcohol Reminds Man Of A Differ...   \n",
       "4  Archaeologists Discover Remnants Of Legendary ...   \n",
       "\n",
       "                                                text  satire/not satire  \\\n",
       "0  big-game hunter baron hugo von urwitz conceded...                  1   \n",
       "1  theorizing that their work most likely represe...                  1   \n",
       "2  in what many are hailing as the most significa...                  1   \n",
       "3  hallandale, fl–randy streeter, a 32-year-old s...                  1   \n",
       "4  archaeologists excavating the train tracks out...                  1   \n",
       "\n",
       "                                      tokenized text  \\\n",
       "0  ['big', 'game', 'hunter', 'baron', 'hugo', 'vo...   \n",
       "1  ['theorizing', 'that', 'their', 'work', 'most'...   \n",
       "2  ['in', 'what', 'many', 'are', 'hailing', 'as',...   \n",
       "3  ['hallandale', 'fl', 'randy', 'streeter', 'a',...   \n",
       "4  ['archaeologists', 'excavating', 'the', 'train...   \n",
       "\n",
       "                                   stopwords removed  \\\n",
       "0  ['big', 'game', 'hunter', 'baron', 'hugo', 'vo...   \n",
       "1  ['theorizing', 'work', 'likely', 'represents',...   \n",
       "2  ['many', 'hailing', 'significant', 'developmen...   \n",
       "3  ['hallandale', 'fl', 'randy', 'streeter', 'yea...   \n",
       "4  ['archaeologists', 'excavating', 'train', 'tra...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['big', 'game', 'hunter', 'baron', 'hugo', 'vo...   \n",
       "1  ['theorizing', 'work', 'likely', 'represents',...   \n",
       "2  ['many', 'hailing', 'significant', 'developmen...   \n",
       "3  ['hallandale', 'fl', 'randy', 'streeter', 'yea...   \n",
       "4  ['archaeologist', 'excavating', 'train', 'trac...   \n",
       "\n",
       "                                      processed text  \n",
       "0  big game hunter baron hugo von urwitz conceded...  \n",
       "1  theorizing work likely represents groundbreaki...  \n",
       "2  many hailing significant development history s...  \n",
       "3  hallandale fl randy streeter year old sale ass...  \n",
       "4  archaeologist excavating train track quarry an...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satire_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d6489",
   "metadata": {},
   "source": [
    "# Build web application interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd4b32e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask-bootstrap in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.7.1)\n",
      "Requirement already satisfied: Flask>=0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask-bootstrap) (1.1.2)\n",
      "Requirement already satisfied: dominate in c:\\programdata\\anaconda3\\lib\\site-packages (from flask-bootstrap) (2.7.0)\n",
      "Requirement already satisfied: visitor in c:\\programdata\\anaconda3\\lib\\site-packages (from flask-bootstrap) (0.1.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-bootstrap) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-bootstrap) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-bootstrap) (1.0.1)\n",
      "Requirement already satisfied: click>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask>=0.8->flask-bootstrap) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-bootstrap) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask-bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8990a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask\n",
    "from flask import Flask, render_template, request, url_for\n",
    "from flask_bootstrap import Bootstrap \n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3c2f59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mnb_vectorizer = open('best_mnb_vectorizer.pkl','rb')\n",
    "best_cv_mnb = open('best_cv_mnb.pkl','rb')\n",
    "\n",
    "best_cv_mnb = joblib.load(best_cv_mnb)\n",
    "best_mnb_vectorizer = joblib.load(best_mnb_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8577f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_satire(text):\n",
    "    result = best_cv_mnb.predict(best_mnb_vectorizer.transform([text]))\n",
    "    return(result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aecc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a661ba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [25/Nov/2022 00:37:34] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [25/Nov/2022 00:37:36] \"\u001b[37mPOST /result HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__ )\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('main.html')\n",
    "\n",
    "@app.route('/about')\n",
    "def index():\n",
    "    return render_template('about.html')\n",
    "\n",
    "@app.route('/result',methods=['POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        \n",
    "        article = request.form['article']\n",
    "        result = predict_satire(article)\n",
    "        return render_template('result.html',satire_prediction = result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "final_textcorpus = [\n",
    "'donald trump sends embarrassing new year eve message disturbing', \n",
    " 'drunk bragging trump staffer started russian collusion investigation', \n",
    " 'sheriff david clarke becomes internet joke threatening poke people eye', \n",
    " 'trump obsessed even obama name coded website image', \n",
    " 'pope francis called donald trump christmas speech', \n",
    " 'racist alabama cop brutalize black boy handcuff graphic image', \n",
    " 'fresh golf course trump lash fbi deputy director james comey', \n",
    " 'trump said insanely racist stuff inside oval office witness back', \n",
    " 'former cia director slam trump un bullying openly suggests acting like dictator tweet', \n",
    " 'watch brand new pro trump ad feature much kissing make sick', \n",
    " 'papa john founder retires figure racism bad business', \n",
    " 'watch paul ryan told u care struggling family living blue state', \n",
    " 'bad news trump mitch mcconnell say repealing obamacare', \n",
    " 'watch lindsey graham trash medium portraying trump kooky forgets word', \n",
    " 'heiress disney empire know gop scammed u shred tax bill'\n",
    "]\n",
    "\n",
    "count_vect = CountVectorizer(max_features=5000, ngram_range=(2,2))\n",
    "\n",
    "#convert to bag of words first\n",
    "all_X = count_vect.fit_transform(final_textcorpus).toarray() -->gives nd array\n",
    "all_y = allnews_data['Real/Fake']\n",
    "#train,test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size = 0.33, random_state = 0)\n",
    "#Fitting the X_train and y_train data into the classifier for training\n",
    "mult_NB_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train test split first, then convert training data to bag of words\n",
    "X = list(df['Text'])\n",
    "y = list(df['Sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1,N))\n",
    "x_train_cv = count_vect.fit_transform(x_train) --> gives matrix\n",
    "x_test_cv = count_vect.transform(x_test)\n",
    "\n",
    "MNB_classifier.fit(x_train_cv, y_train)\n",
    "y_pred = MNB_classifier.predict(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "154763cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'former cia director slam trump'\n",
    "\n",
    "\n",
    "y_pred = classifier.predict(vectorizer.transform([text]))\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
